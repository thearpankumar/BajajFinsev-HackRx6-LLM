# ==============================================================================
# BajajFinsev RAG System - Environment Configuration
# Copy this file to .env and update with your actual values
# ==============================================================================

# ==============================================================================
# API AUTHENTICATION (REQUIRED)
# ==============================================================================
# Your API key for accessing the BajajFinsev RAG API
API_KEY=123456

# OpenAI API Key (REQUIRED) - Get from https://platform.openai.com/api-keys
OPENAI_API_KEY=sk-proj-your_openai_api_key_here

# Google AI API Key (REQUIRED) - Get from https://aistudio.google.com/app/apikey
GOOGLE_API_KEY=your_google_api_key_here

# ==============================================================================
# EMBEDDING CONFIGURATION
# ==============================================================================
# OpenAI embedding model to use
EMBEDDING_MODEL=text-embedding-3-small

# Embedding dimensions (must match the model)
EMBEDDING_DIMENSIONS=1536

# Batch size for embedding generation (higher = faster but more memory)
EMBEDDING_BATCH_SIZE=100

# ==============================================================================
# DOCUMENT PROCESSING CONFIGURATION
# ==============================================================================
# Size of text chunks for processing
CHUNK_SIZE=1000

# Overlap between chunks to maintain context
CHUNK_OVERLAP=200

# Maximum document size in MB
MAX_DOCUMENT_SIZE_MB=100

# Threshold for large document processing (bytes)
LARGE_DOCUMENT_THRESHOLD=20971520

# Character threshold for hierarchical processing
LARGE_DOCUMENT_CHAR_THRESHOLD=500000

# Enable hierarchical processing for large documents
ENABLE_HIERARCHICAL_PROCESSING=true

# ==============================================================================
# RETRIEVAL CONFIGURATION
# ==============================================================================
# Maximum chunks to retrieve from vector database
MAX_CHUNKS_PER_QUERY=10

# Top-K chunks for retrieval
TOP_K_RETRIEVAL=10

# Top-K chunks after re-ranking
RERANK_TOP_K=5

# Maximum sections per query for hierarchical processing
MAX_SECTIONS_PER_QUERY=3

# ==============================================================================
# PERFORMANCE OPTIMIZATION
# ==============================================================================
# Enable fast mode (recommended for production)
FAST_MODE=true

# Enable cross-encoder re-ranking (slower but more accurate)
ENABLE_RERANKING=false

# Maximum chunks used for answer generation
MAX_CHUNKS_FOR_GENERATION=5

# Enable parallel question processing
PARALLEL_PROCESSING=true

# Maximum questions to process in parallel
MAX_PARALLEL_QUESTIONS=40

# Batch size for parallel processing
QUESTION_BATCH_SIZE=10

# Number of parallel batches for embedding
PARALLEL_BATCHES=3

# ==============================================================================
# AI MODEL CONFIGURATION
# ==============================================================================
# OpenAI model for text generation
OPENAI_GENERATION_MODEL=gpt-4o-mini

# Maximum tokens for OpenAI responses
OPENAI_MAX_TOKENS=4000

# Temperature for OpenAI generation (0.0-1.0)
OPENAI_TEMPERATURE=0.1

# Google AI model for query processing
GOOGLE_MODEL=gemini-2.0-flash-exp

# ==============================================================================
# VECTOR DATABASE CONFIGURATION
# ==============================================================================
# Path to vector database storage
VECTOR_DB_PATH=./vector_db

# Vector database dimension (must match embedding model)
VECTOR_DIMENSION=1536

# ==============================================================================
# CACHING CONFIGURATION
# ==============================================================================
# Enable document caching for better performance
ENABLE_DOCUMENT_CACHE=true

# Cache expiry time in hours
CACHE_EXPIRY_HOURS=24

# Cache TTL in seconds
CACHE_TTL=3600

# ==============================================================================
# STREAMING AND RESPONSE CONFIGURATION
# ==============================================================================
# Enable streaming responses (experimental)
ENABLE_STREAMING_RESPONSES=false

# ==============================================================================
# HYBRID SEARCH CONFIGURATION
# ==============================================================================
# Weight for dense (vector) search in hybrid search
DENSE_WEIGHT=0.7

# Weight for sparse (BM25) search in hybrid search
SPARSE_WEIGHT=0.3

# BM25 parameters
BM25_K1=1.2
BM25_B=0.75

# Similarity threshold for vector search
SIMILARITY_THRESHOLD=0.7

# ==============================================================================
# SYSTEM CONFIGURATION
# ==============================================================================
# Python path
PYTHONPATH=/app

# Python optimization
PYTHONUNBUFFERED=1
PYTHONDONTWRITEBYTECODE=1

# Disable tokenizers parallelism warnings
TOKENIZERS_PARALLELISM=false

# Thread configuration for performance
OMP_NUM_THREADS=4
MKL_NUM_THREADS=4

# ==============================================================================
# SERVER CONFIGURATION
# ==============================================================================
# Maximum concurrent requests
MAX_CONCURRENT_REQUESTS=10

# Request timeout in seconds
REQUEST_TIMEOUT=300

# ==============================================================================
# DEVELOPMENT/DEBUG SETTINGS
# ==============================================================================
# Enable debug logging (set to true for development)
DEBUG=false

# Log level (DEBUG, INFO, WARNING, ERROR)
LOG_LEVEL=INFO

# ==============================================================================
# OPTIONAL: DATABASE CONFIGURATION (if using external DB)
# ==============================================================================
# DATABASE_URL=postgresql://user:password@localhost:5432/bajaj_finsev_db

# ==============================================================================
# OPTIONAL: EXTERNAL SERVICE KEYS (if needed)
# ==============================================================================
# GROQ_API_KEY=your_groq_api_key_here
# PINECONE_API_KEY=your_pinecone_api_key_here
# PINECONE_INDEX_NAME=bajaj-legal-docs
# AWS_ACCESS_KEY_ID=your_aws_access_key_here
# AWS_SECRET_ACCESS_KEY=your_aws_secret_key_here
# AWS_REGION=ap-south-1

# ==============================================================================
# USAGE NOTES:
# 
# 1. REQUIRED VARIABLES:
#    - API_KEY: For API authentication
#    - OPENAI_API_KEY: For embeddings and text generation
#    - GOOGLE_API_KEY: For query processing
#
# 2. PERFORMANCE TUNING:
#    - For speed: FAST_MODE=true, ENABLE_RERANKING=false
#    - For accuracy: FAST_MODE=false, ENABLE_RERANKING=true
#    - For high load: Increase MAX_PARALLEL_QUESTIONS and QUESTION_BATCH_SIZE
#
# 3. RESOURCE OPTIMIZATION:
#    - Reduce MAX_CHUNKS_FOR_GENERATION for faster responses
#    - Increase EMBEDDING_BATCH_SIZE for faster embedding (more memory)
#    - Enable ENABLE_DOCUMENT_CACHE for repeated document processing
#
# 4. DEVELOPMENT:
#    - Set DEBUG=true for detailed logging
#    - Use smaller values for parallel processing during development
#
# ==============================================================================
